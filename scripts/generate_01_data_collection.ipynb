
import json
import os

notebook_content = {
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01. Data Collection\n",
    "\n",
    "## Overview\n",
    "This notebook handles the retrieval of raw data from the [Elden Ring Fan API](https://docs.eldenring.fanapis.com/docs/).\n",
    "We fetch data for multiple categories (items, weapons, NPCs, etc.) and cache them locally in `data/raw/` to avoid redundant network requests.\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": None,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "# Configuration\n",
    "BASE_URL = \"https://eldenring.fanapis.com/api\"\n",
    "ENDPOINTS = [\n",
    "    \"items\",\n",
    "    \"weapons\",\n",
    "    \"npcs\",\n",
    "    \"locations\",\n",
    "    \"bosses\",\n",
    "    \"armors\",\n",
    "    \"talismans\",\n",
    "    \"incantations\",\n",
    "    \"creatures\",\n",
    "    \"shields\"\n",
    "]\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = Path(\"../data/raw\")\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Data directory: {DATA_DIR.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching Functions\n",
    "We define helper functions to handle pagination and rate limiting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": None,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_endpoint(endpoint: str) -> List[dict]:\n",
    "    \"\"\"Fetch all pages from a given API endpoint.\"\"\"\n",
    "    page = 0\n",
    "    results = []\n",
    "    page_size = 100\n",
    "    \n",
    "    print(f\"Fetching {endpoint}...\")\n",
    "    \n",
    "    while True:\n",
    "        params = {\"limit\": page_size, \"page\": page}\n",
    "        url = f\"{BASE_URL}/{endpoint}\"\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, params=params, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            payload = response.json()\n",
    "            data = payload.get(\"data\", [])\n",
    "            \n",
    "            if not data:\n",
    "                break\n",
    "            \n",
    "            results.extend(data)\n",
    "            total = payload.get(\"total\")\n",
    "            \n",
    "            print(f\"  Page {page}: Got {len(data)} items. (Total so far: {len(results)}) \")\n",
    "            \n",
    "            if total is not None and len(results) >= int(total):\n",
    "                break\n",
    "            \n",
    "            page += 1\n",
    "            time.sleep(0.2) # Be polite to the API\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {endpoint} page {page}: {e}\")\n",
    "            break\n",
    "            \n",
    "    return results\n",
    "\n",
    "def save_data(endpoint: str, data: List[dict]):\n",
    "    \"\"\"Save fetched data to JSON file.\"\"\"\n",
    "    filepath = DATA_DIR / f\"{endpoint}.json\"\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "    print(f\"Saved {len(data)} records to {filepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution\n",
    "Iterate through all endpoints and download the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": None,
   "metadata": {},
   "outputs": [],
   "source": [
    "for endpoint in ENDPOINTS:\n",
    "    data = fetch_endpoint(endpoint)\n",
    "    if data:\n",
    "        save_data(endpoint, data)\n",
    "    else:\n",
    "        print(f\"Warning: No data found for {endpoint}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wiki Scraping\n",
    "To augment our data, we scrape the Fextralife wiki for cross-references."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": None,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.parse\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "WIKI_BASE_URL = \"https://eldenring.wiki.fextralife.com\"\n",
    "WIKI_DATA_DIR = DATA_DIR / \"wiki_html\"\n",
    "WIKI_DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "def get_wiki_url(name):\n",
    "    # Simple heuristic: replace spaces with +\n",
    "    clean_name = name.replace(\" \", \"+\").replace(\"'\", \"%27\")\n",
    "    return f\"{WIKI_BASE_URL}/{clean_name}\"\n",
    "\n",
    "def scrape_wiki_references(nodes_data):\n",
    "    # We focus on NPCs and Bosses first as they are central hubs\n",
    "    target_nodes = [n for n in nodes_data if n.get('type') in ['npc', 'boss']]\n",
    "    print(f\"Targeting {len(target_nodes)} wiki pages...\")\n",
    "    \n",
    "    session = requests.Session()\n",
    "    session.headers.update({\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    })\n",
    "    \n",
    "    for i, node in enumerate(target_nodes):\n",
    "        name = node['name']\n",
    "        url = get_wiki_url(name)\n",
    "        \n",
    "        try:\n",
    "            # Check if we already have it\n",
    "            cache_path = WIKI_DATA_DIR / f\"{name.replace(' ', '_').replace(':', '')}.html\"\n",
    "            if cache_path.exists():\n",
    "                continue\n",
    "                \n",
    "            print(f\"Fetching {name} ({i+1}/{len(target_nodes)})...\")\n",
    "            response = session.get(url, timeout=10)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                with open(cache_path, 'w', encoding='utf-8') as f:\n",
    "                    f.write(response.text)\n",
    "                time.sleep(1.0) # Polite delay\n",
    "            else:\n",
    "                print(f\"Failed to fetch {name}: {response.status_code}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {name}: {e}\")\n",
    "            \n",
    "    print(\"Wiki scraping complete.\")\n",
    "\n",
    "# Load the data we just fetched to get the list of nodes\n",
    "all_nodes = []\n",
    "for endpoint in ENDPOINTS:\n",
    "    filepath = DATA_DIR / f\"{endpoint}.json\"\n",
    "    if filepath.exists():\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            all_nodes.extend(json.load(f))\n",
    "\n",
    "# Uncomment to run scraping\n",
    "# scrape_wiki_references(all_nodes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

with open('c:/social_graphs_project/notebooks/01_data_collection.ipynb', 'w', encoding='utf-8') as f:
    json.dump(notebook_content, f, indent=1)
