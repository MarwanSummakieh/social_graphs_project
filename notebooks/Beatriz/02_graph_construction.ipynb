{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce2fff8a",
   "metadata": {},
   "source": [
    "# 02. Graph Construction (Week 2)\n",
    "\n",
    "## Week 2: Graph Theory & NetworkX\n",
    "\n",
    "This notebook processes the raw API data into a multimodal graph that captures the structural mechanics of the game.\n",
    "\n",
    "### Course Concepts Applied\n",
    "-   **Nodes & Edges:** Defining what constitutes a node and an edge.\n",
    "-   **Attributes:** Attaching metadata to nodes for later analysis.\n",
    "-   **Graph Construction:** Building the network structure from tabular data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae4e8f4",
   "metadata": {},
   "source": [
    "### 1. Data for the graph\n",
    "\n",
    "For the graph, we utilize data from the Elden Ring API and scrapped data from the Elden Ring. \n",
    "\n",
    "From the API we get data about:\n",
    "- items\n",
    "- weapons\n",
    "- NPCs \n",
    "- locations\n",
    "- bosses\n",
    "- armors\n",
    "- talismans\n",
    "- incantations\n",
    "\n",
    "From the Wiki page we get data about:\n",
    "- armors\n",
    "- bosses\n",
    "- npcs\n",
    "- weapons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d103db5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Root: C:\\Users\\biagu\\Documents\\GitHub\\social_graphs_project\n",
      "API Data: C:\\Users\\biagu\\Documents\\GitHub\\social_graphs_project\\data\\raw\n",
      "Wiki Data: C:\\Users\\biagu\\Documents\\GitHub\\social_graphs_project\\data\\scraped\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Set, Tuple\n",
    "from dataclasses import dataclass, asdict\n",
    "from itertools import combinations\n",
    "import os\n",
    "\n",
    "# ==========================================\n",
    "# 1. CONFIGURATION & SETUP\n",
    "# ==========================================\n",
    "\n",
    "# Absolute paths as confirmed working\n",
    "project_root = Path(\"C:/Users/biagu/Documents/GitHub/social_graphs_project\")\n",
    "\n",
    "API_DIR = Path(\"C:/Users/biagu/Documents/GitHub/social_graphs_project/data/raw\")\n",
    "WIKI_DIR = Path(\"C:/Users/biagu/Documents/GitHub/social_graphs_project/data/scraped\")\n",
    "PROCESSED_DIR = Path(\"C:/Users/biagu/Documents/GitHub/social_graphs_project/data/processed\")\n",
    "\n",
    "# Create output directory\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Project Root: {project_root}\")\n",
    "print(f\"API Data: {API_DIR}\")\n",
    "print(f\"Wiki Data: {WIKI_DIR}\")\n",
    "\n",
    "# Map Wiki filenames to Node Types\n",
    "WIKI_FILES = {\n",
    "    \"armor.json\": \"armor\",\n",
    "    \"bosses.json\": \"boss\",\n",
    "    \"npcs.json\": \"npc\",\n",
    "    \"weapons.json\": \"weapon\"\n",
    "}\n",
    "\n",
    "# API Endpoints\n",
    "API_ENDPOINTS = [\n",
    "    \"items\", \"weapons\", \"npcs\", \"locations\", \n",
    "    \"bosses\", \"armors\", \"talismans\", \"incantations\"\n",
    "]\n",
    "\n",
    "@dataclass\n",
    "class NodeRecord:\n",
    "    node_id: str\n",
    "    node_type: str\n",
    "    name: str\n",
    "    description: str\n",
    "    source: str\n",
    "    metadata: dict\n",
    "\n",
    "@dataclass\n",
    "class EdgeRecord:\n",
    "    source: str\n",
    "    target: str\n",
    "    relationship: str\n",
    "    edge_type: str\n",
    "    metadata: dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2e1a3b",
   "metadata": {},
   "source": [
    "### 2. Helper Functions & Data Loading\n",
    "This section defines functions to normalize names, clean text, and safely load JSON files. It also contains the primary logic for merging the structured API data with the unstructured Wiki data into a single NodeRecord format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9344c484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 2. HELPER FUNCTIONS \n",
    "# ==========================================\n",
    "\n",
    "def normalize_name(name):\n",
    "    if not name: return None\n",
    "    # Lowercase, strip, remove special chars\n",
    "    return name.lower().strip().replace(\"'\", \"\").replace(\"-\", \" \")\n",
    "\n",
    "def clean_text(value: str) -> str:\n",
    "    if not value: return \"\"\n",
    "    return re.sub(r\"\\s+\", \" \", value).strip()\n",
    "\n",
    "def explode_locations(raw_value: str) -> List[str]:\n",
    "    \"\"\"Splits location strings into individual location names.\"\"\"\n",
    "    if not raw_value: return []\n",
    "    if not isinstance(raw_value, str): return []\n",
    "    \n",
    "    seps = [\",\", \"/\", \" and \", \" & \", \";\"]\n",
    "    parts = [raw_value]\n",
    "    for sep in seps:\n",
    "        nxt = []\n",
    "        for part in parts:\n",
    "            nxt.extend(part.split(sep))\n",
    "        parts = nxt\n",
    "    return [p.strip() for p in parts if p.strip()]\n",
    "\n",
    "def load_json_safe(path):\n",
    "    if not path.exists():\n",
    "        print(f\"  ❌ File not found: {path}\")\n",
    "        return []\n",
    "    try:\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            return json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"  ⚠️ Error loading {path.name}: {e}\")\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23191b0a",
   "metadata": {},
   "source": [
    "### 3. Nodes and Edge Generation\n",
    "\n",
    "It contains the primary logic for merging the structured API data with the unstructured Wiki data into a single NodeRecord format.\n",
    "\n",
    "Here we construct the relationships (Edges) between nodes. We use three specific techniques:\n",
    "\n",
    "Explicit Edges: Linking items to locations via \"Found In\" metadata and \"Drops\".\n",
    "\n",
    "Shared Locations: Linking characters (Bosses/NPCs) if they inhabit the same location bucket.\n",
    "\n",
    "Mentions (NLP): Searching descriptions for mentions of other nodes (Lore connections)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e051e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 3. DATA LOADING & MERGING\n",
    "# ==========================================\n",
    "\n",
    "def build_merged_nodes():\n",
    "    nodes_map = {} \n",
    "\n",
    "    print(\"\\n--- Loading API Data ---\")\n",
    "    for endpoint in API_ENDPOINTS:\n",
    "        data = load_json_safe(API_DIR / f\"{endpoint}.json\")\n",
    "        for item in data:\n",
    "            name = item.get(\"name\")\n",
    "            if not name: continue\n",
    "            norm_name = normalize_name(name)\n",
    "            \n",
    "            n_type = endpoint[:-1] if endpoint.endswith('s') else endpoint\n",
    "            nodes_map[norm_name] = NodeRecord(\n",
    "                node_id=item.get(\"id\") or f\"api_{norm_name.replace(' ', '_')}\",\n",
    "                node_type=n_type,\n",
    "                name=name,\n",
    "                description=clean_text(item.get(\"description\") or item.get(\"effect\")),\n",
    "                source=\"api\",\n",
    "                metadata={k:v for k,v in item.items() if k not in ['id', 'name', 'description', 'effect']}\n",
    "            )\n",
    "\n",
    "    print(\"\\n--- Loading Wiki Data ---\")\n",
    "    for filename, n_type in WIKI_FILES.items():\n",
    "        data = load_json_safe(WIKI_DIR / filename)\n",
    "        for item in data:\n",
    "            name = item.get(\"name\")\n",
    "            if not name: continue\n",
    "            norm_name = normalize_name(name)\n",
    "            \n",
    "            infobox = item.get(\"infobox\", {})\n",
    "            wiki_desc = clean_text(item.get(\"description\"))\n",
    "            \n",
    "            if norm_name in nodes_map:\n",
    "                existing = nodes_map[norm_name]\n",
    "                if wiki_desc and wiki_desc not in existing.description:\n",
    "                    existing.description = (existing.description + \" \" + wiki_desc).strip()\n",
    "                existing.metadata.update(infobox)\n",
    "                existing.metadata['wiki_url'] = item.get(\"url\")\n",
    "                existing.source = \"merged\"\n",
    "            else:\n",
    "                nodes_map[norm_name] = NodeRecord(\n",
    "                    node_id=f\"wiki_{norm_name.replace(' ', '_')}\",\n",
    "                    node_type=n_type,\n",
    "                    name=name,\n",
    "                    description=wiki_desc,\n",
    "                    source=\"wiki\",\n",
    "                    metadata=infobox\n",
    "                )\n",
    "    \n",
    "    return list(nodes_map.values())\n",
    "\n",
    "# ==========================================\n",
    "# 4. EDGE GENERATION \n",
    "# ==========================================\n",
    "\n",
    "def build_edges(nodes: List[NodeRecord]):\n",
    "    edges = []\n",
    "    node_lookup = {normalize_name(n.name): n.node_id for n in nodes}\n",
    "    location_ids = {n.node_id for n in nodes if n.node_type == 'location'}\n",
    "    \n",
    "    print(\"\\n--- Generating Edges ---\")\n",
    "    \n",
    "    # 1. EXPLICIT EDGES (Location & Drops)\n",
    "    print(\"Processing Explicit Edges (Found In / Drops)...\")\n",
    "    for node in nodes:\n",
    "        # A. Location Edges\n",
    "        # Check both API 'location' and Wiki 'Location' fields\n",
    "        loc_str = node.metadata.get(\"location\") or node.metadata.get(\"Location\")\n",
    "        if loc_str:\n",
    "            # Use explode_locations to handle lists like \"Limgrave, Stormhill\"\n",
    "            possible_locs = explode_locations(str(loc_str))\n",
    "            for loc_name in possible_locs:\n",
    "                norm_loc = normalize_name(loc_name)\n",
    "                # Try direct match\n",
    "                target_id = node_lookup.get(norm_loc)\n",
    "                \n",
    "                if not target_id:\n",
    "                     for known_loc_name, known_loc_id in node_lookup.items():\n",
    "                        if known_loc_id in location_ids and known_loc_name in norm_loc:\n",
    "                            target_id = known_loc_id\n",
    "                            break\n",
    "                \n",
    "                if target_id and target_id in location_ids:\n",
    "                    edges.append(EdgeRecord(node.node_id, target_id, \"found_in\", \"location\", {}))\n",
    "\n",
    "        # B. Drops Edges\n",
    "        drops = node.metadata.get(\"drops\") or []\n",
    "        if isinstance(drops, list):\n",
    "            for drop_name in drops:\n",
    "                drop_id = node_lookup.get(normalize_name(drop_name))\n",
    "                if drop_id:\n",
    "                    edges.append(EdgeRecord(node.node_id, drop_id, \"drops\", \"drop\", {}))\n",
    "\n",
    "    # 2. SHARED LOCATION EDGES\n",
    "    # Connects Bosses/NPCs that are in the same location bucket\n",
    "    print(\"Processing Shared Location Edges...\")\n",
    "    loc_buckets: Dict[str, List[str]] = {}\n",
    "    \n",
    "    for node in nodes:\n",
    "        # Only group Characters (Bosses, NPCs)\n",
    "        if node.node_type not in ['boss', 'npc']:\n",
    "            continue\n",
    "            \n",
    "        loc_str = node.metadata.get(\"location\") or node.metadata.get(\"Location\")\n",
    "        if loc_str:\n",
    "            for loc in explode_locations(str(loc_str)):\n",
    "                norm = normalize_name(loc)\n",
    "                if norm:\n",
    "                    loc_buckets.setdefault(norm, []).append(node.node_id)\n",
    "\n",
    "    for loc_name, node_ids in loc_buckets.items():\n",
    "        # If multiple chars are in this location, connect them\n",
    "        if len(node_ids) > 1:\n",
    "            # Sort to ensure unique pairs\n",
    "            for a, b in combinations(sorted(set(node_ids)), 2):\n",
    "                edges.append(EdgeRecord(a, b, \"share_location\", \"share_location\", {\"location\": loc_name}))\n",
    "\n",
    "    # 3. MENTIONS\n",
    "    # Check if ANY node name appears in ANY node description\n",
    "    print(\"Processing Mention Edges (NLP)...\")\n",
    "    \n",
    "    # Filter short names to avoid noise (e.g., \"Map\", \"Key\")\n",
    "    searchable_names = {name: nid for name, nid in node_lookup.items() if len(name) >= 5}\n",
    "    \n",
    "    for node in nodes:\n",
    "        desc = (node.description or \"\").lower()\n",
    "        if not desc: continue\n",
    "        \n",
    "        # Check against all searchable names\n",
    "        for target_name, target_id in searchable_names.items():\n",
    "            if target_id == node.node_id: continue # No self-loops\n",
    "            \n",
    "            if target_name in desc:\n",
    "                edges.append(EdgeRecord(node.node_id, target_id, \"mentions\", \"mention\", {}))\n",
    "\n",
    "    return edges\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2be685",
   "metadata": {},
   "source": [
    "### 4. Execution & Export\n",
    "\n",
    "Runs the pipeline and saves the nodes_enriched.csv and edges_enriched.csv files to the processed directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f4ba577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Loading API Data ---\n",
      "\n",
      "--- Loading Wiki Data ---\n",
      "Nodes Loaded: 2410\n",
      "\n",
      "--- Generating Edges ---\n",
      "Processing Explicit Edges (Found In / Drops)...\n",
      "Processing Shared Location Edges...\n",
      "Processing Mention Edges (NLP)...\n",
      "Edges Created: 4805\n",
      "✅ Saved enriched graph to C:\\Users\\biagu\\Documents\\GitHub\\social_graphs_project\\data\\processed\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 5. EXECUTION\n",
    "# ==========================================\n",
    "\n",
    "nodes = build_merged_nodes()\n",
    "\n",
    "if not nodes:\n",
    "    print(\"❌ Critical Error: No nodes loaded. Verify your paths!\")\n",
    "else:\n",
    "    print(f\"Nodes Loaded: {len(nodes)}\")\n",
    "    \n",
    "    edges = build_edges(nodes)\n",
    "    print(f\"Edges Created: {len(edges)}\")\n",
    "\n",
    "    node_df = pd.DataFrame([asdict(n) for n in nodes])\n",
    "    edge_df = pd.DataFrame([asdict(e) for e in edges])\n",
    "\n",
    "    # JSON serialize metadata\n",
    "    if 'metadata' in node_df.columns:\n",
    "        node_df['metadata'] = node_df['metadata'].apply(lambda x: json.dumps(x, ensure_ascii=False))\n",
    "    if 'metadata' in edge_df.columns:\n",
    "        edge_df['metadata'] = edge_df['metadata'].apply(lambda x: json.dumps(x, ensure_ascii=False))\n",
    "\n",
    "    node_df.to_csv(PROCESSED_DIR / \"nodes_enriched.csv\", index=False)\n",
    "    edge_df.to_csv(PROCESSED_DIR / \"edges_enriched.csv\", index=False)\n",
    "    print(f\"✅ Saved enriched graph to {PROCESSED_DIR}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
